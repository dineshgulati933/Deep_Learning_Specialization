# Deep_Learning_Specialization by DeepLearning.ai on Coursera

**IMPORTANT:** **_Before accessing this repo, be sure to abide by the honest code of coursera which means you are not using this repo code to submit as your own to pass the lab assignments. This repo is only for information purpose or audience who audit this course on coursera. All notenooks and lecture notes are property of deeplearning and may be deleted if objected._**

**Master Deep Learning, and Break into AI**

- Organization: [deeplearning.ai](https://www.deeplearning.ai)
- Coursera: [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)
- Instructor: [Andrew Ng](http://www.andrewng.org/)


## Introduction

This repo contains all the lab assignments as well as lecture slides for deep learning specialization by deeplearning.ai on coursera. All the code base and images, are taken from [Deep Learning Specialization on Coursera](https://www.coursera.org/specializations/deep-learning).

*In **five courses**, you will learn the foundations of Deep Learning, understand how to build neural networks, and learn how to lead successful machine learning projects. You will learn about Convolutional networks, RNNs, LSTM, Adam, Dropout, BatchNorm, Xavier/He initialization, and more. You will work on case studies from healthcare, autonomous driving, sign language reading, music generation, and natural language processing. You will master not only the theory, but also see how it is applied in industry. You will practice all these ideas in Python and in TensorFlow, which we will teach.*




## 1. Neural Networks and Deep Learning

### Assignments

- [Week 2 - 
Python Basics with Numpy](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_1/Week_2/Lab/W2A1/Python_Basics_with_Numpy.ipynb)
- [Week 2 - 
Logistic Regression with a Neural Network mindset](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_1/Week_2/Lab/W2A2/Logistic_Regression_with_a_Neural_Network_mindset.ipynb)
- [Week 3 - Planar data classification with one hidden layer](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_1/Week_3/Lab/W3A1/Planar_data_classification_with_one_hidden_layer.ipynb)
- [Week 4 - Building your Deep Neural Network - Step by Step](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_1/Week_4/Lab/W4A1/Building_your_Deep_Neural_Network_Step_by_Step.ipynb)
- [Week 4 - Deep Neural Network for Image Classification: Application](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_1/Week_4/Lab/W4A2/Deep%20Neural%20Network%20-%20Application.ipynb)

### Lectures / Notes

- [Week 1](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_1/Week_1/Lecture/C1_W1.pdf) --> Introduction, NN, Why Deep learning
- [Week 2](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_1/Week_2/Lecture/C1_W2.pdf) --> Logistic regression, Gradient Descent, Derivatives, Vectorization, Python Broadcasting
- [Week 3](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_1/Week_3/Lecture/C1_W3.pdf) --> NN, Activation function, Backpropagate, Random Initialization
- [Week 4](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_1/Week_4/Lecture/C1_W4.pdf) --> Deep L-layer Neural network, Matrix dimension right, Why Deep representation, Building blocks of NN, Parameters vs Hyperparameters, Relationship with brain



## 2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization


### Assignments

- [Week 1 Initialization](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_2/Week_1/Labs/W1A1/Initialization.ipynb)
- [Week 1 Regularization](https://github.com/saqemlas/deep-learning-specialization/tree/main/2_ImprovingDeepNeuralNetworksHyperparameterTuningRegularizationAndOptimization/week_1/Assignment/Regularization.ipynb)
- [Week 1 Gradient Checking](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_2/Week_1/Labs/W1A3/Gradient_Checking.ipynb)
- [Week 2 Optimization Methods](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_2/Week_2/Lab/W2A1/Optimization_methods.ipynb)
- [Week 3 TensorFlow Tutorial](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_2/Week_3/Lab/W3A1/Tensorflow_introduction.ipynb)

### Lectures / Notes

- [Week 1](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_2/Week_1/Lectures/C2_W1.pdf) --> Train/Dev/Test set, Bias/Variance, Regularization, Why regularization, Dropout, Normalizing inputs, vanishing/exploding gradients, Gradient checking
- [Week 2](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_2/Week_2/Lectures/C2_W2.pdf) --> Mini-batch, Exponentially weighted average, GD with momentum, RMSProp, Adam optimizer, Learning rate decay, Local optima problem, Plateaus problem
- [Week 3](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_2/Week_3/Lectures/C2_W3.pdf) --> Tuning process, Picking hyperparameter, Normalizing activations, Softmax regression, Deep learning programming framework 



# 3. Structuring Machine Learning Projects


### Lectures / Notes

- [Week 1](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_3/Week_1/C3_W1.pdf) --> Why ML Strategy?, Orthogonalization, Single number evaluation metric, Satisficing and optimizing metrics, Train/dev/test distribution, Human level performance, Avoidable bias
- [Week 2](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_3/Week_2/C3_W2.pdf) --> Error analysis, Incorrectly labeled data, Data augmentation, Transfer learning, Multitask learning, End-to-End Deep learning


# 4. Convolutional Neural Networks

### Assignments

- [Week 1 - Convolutional Neural Networks: Step By Step](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_4/Week_1/Labs/W1A1/Convolution_model_Step_by_Step_v1.ipynb)
- [Week 1 - Convolutional Neural Networks: Application](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_4/Week_1/Labs/W1A2/Convolution_model_Application.ipynb)
- [Week 2 - ResNets](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_4/Week_2/Labs/W2A1/Residual_Networks.ipynb)
- [Week 2 - Transfer Learning with MobileNet](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_4/Week_2/Labs/W2A2/Transfer_learning_with_MobileNet_v1.ipynb)
- [Week 3 - Car detection for Autonomous Driving](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_4/Week_3/Labs/W3A1/Autonomous_driving_application_Car_detection.ipynb)
- [Week 3 - Image Segmentation Unet](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_4/Week_3/Labs/W3A2/Image_segmentation_Unet_v2.ipynb)
- [Week 4 - Face Recognition](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_4/Week_4/Labs/W4A1/Face_Recognition.ipynb)
- [Week 4 - Neural Style Transfer](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_4/Week_4/Labs/W4A2/Art_Generation_with_Neural_Style_Transfer.ipynb)


### Lectures / Notes

- [Week 1](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_4/Week_1/Lectures/C4_W1.pdf) --> Computer vision, Edge detection, Padding, Strided convolution, Convolutions over volume, Pooling layers, CNN, Why CNN?
- [Week 2](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_4/Week_2/Lectures/C4_W2.pdf) --> LeNet-5, AlexNet, VGG-16, ResNets, 1x1 convolutions, InceptionNet, Data augmentation
- [Week 3](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_4/Week_3/Lectures/C4_W3.pdf) --> Object localization, Landmark detection, Object detection, Sliding window, Bounding box prediction, Intersection over union(IOU), Non-max suppression, Anchor box, YOLO algorithm
- [Week 4](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_4/Week_4/Lectures/C4_W4.pdf) --> Face recognition, One-shot learning, Siamese network, Neural style transfer



# 5. Sequence Models



### Assignments

- [Week 1 - Building a Recurrent Neural Network - Step by Step](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_5/Week_1/Labs/W1A1/Building_a_Recurrent_Neural_Network_Step_by_Step.ipynb)
- [Week 1 - Character level language model - Dinosaurus Island](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_5/Week_1/Labs/W1A2/Dinosaurus_Island_Character_level_language_model.ipynb)
- [Week 1 - Improvise a Jazz Solo with an LSTM Network](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_5/Week_1/Labs/W1A3/Improvise_a_Jazz_Solo_with_an_LSTM_Network_v4.ipynb)
- [Week 2 - Word Vector Representation](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_5/Week_2/Labs/W2A1/Operations_on_word_vectors_v2a.ipynb)
- [Week 2 - Emojify](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_5/Week_2/Labs/W2A2/Emoji_v3a.ipynb)
- [Week 3 - Neural Machine Translation](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_5/Week_3/Labs/W3A1/Neural_machine_translation_with_attention_v4a.ipynb)
- [Week 3 - Trigger word detection](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_5/Week_3/Labs/W3A2/Trigger_word_detection_v2a.ipynb)
- [Week 4 - Transformer](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_5/Week_4/Labs/W4A1/C5_W4_A1_Transformer_Subclass_v1.ipynb)


### Lectures / Notes

- [Week 1](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_5/Week_1/Lectures/C5_W1.pdf) --> Discover recurrent neural networks, a type of model that performs extremely well on temporal data, and several of its variants, including LSTMs, GRUs and Bidirectional RNNs,
- [Week 2](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_5/Week_2/Lectures/C5_W2.pdf) --> word vector representations and embedding layers, train recurrent neural networks with outstanding performance across a wide variety of applications, including sentiment analysis, named entity recognition and neural machine translation
- [Week 3](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_5/Week_3/Lectures/C5_W3.pdf) --> Augment your sequence models using an attention mechanism, an algorithm that helps your model decide where to focus its attention given a sequence of inputs. Then, explore speech recognition and how to deal with audio data
- [Week 4](https://github.com/dineshgulati933/Deep_Learning_Specialization/blob/main/Course_5/Week_4/Lectures/C5_W4.pdf) --> Natural language processing with deep learning is a powerful combination. Using word vector representations and embedding layers, train recurrent neural networks with outstanding performance across a wide variety of applications, including sentiment analysis, named entity recognition and neural machine translation.